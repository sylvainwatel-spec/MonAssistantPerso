import customtkinter as ctk
from tkinter import messagebox, filedialog
from utils.llm_connector import LLMConnectionTester
from utils.web_scraper import WebScraper
from utils.results_manager import ResultsManager
import threading
import datetime
import os
import logging

class ChatFrame(ctk.CTkFrame):
    def __init__(self, master, app, assistant_data):
        super().__init__(master, fg_color="transparent")
        self.app = app
        self.assistant = assistant_data
        self.history = [] # Liste pour stocker l'historique des messages
        
        # Header avec bouton retour
        header_frame = ctk.CTkFrame(self, fg_color="transparent", height=60)
        header_frame.pack(fill="x", padx=20, pady=10)
        header_frame.pack_propagate(False)
        
        btn_back = ctk.CTkButton(
            header_frame,
            text="< Retour",
            width=100,
            height=32,
            fg_color=("#3B8ED0", "#1F6AA5"),
            corner_radius=16,
            command=self.app.show_list,
        )
        btn_back.pack(side="left")
        
        title = ctk.CTkLabel(
            header_frame,
            text=f"üí¨ Chat avec {self.assistant.get('name', 'Assistant')}",
            font=("Arial", 20, "bold")
        )
        title.pack(side="left", padx=20)
        
        # Bouton Export Excel
        btn_export = ctk.CTkButton(
            header_frame,
            text="üì• Export Excel",
            width=120,
            height=32,
            fg_color=("#2E7D32", "#1B5E20"), # Vert fonc√©
            corner_radius=16,
            command=self.export_to_excel
        )
        btn_export.pack(side="right", padx=10)
        
        # Indicateur de provider
        provider_label = ctk.CTkLabel(
            header_frame,
            text=f"ü§ñ {self.assistant.get('provider', 'Non d√©fini')}",
            font=("Arial", 12),
            text_color="gray"
        )
        provider_label.pack(side="right", padx=10)
        
        # Zone de chat
        self.chat_area = ctk.CTkTextbox(
            self,
            font=("Arial", 13),
            wrap="word"
        )
        self.chat_area.pack(fill="both", expand=True, padx=20, pady=(0, 10))
        self.chat_area.configure(state="disabled")
        
        # Barre de progression (cach√©e par d√©faut)
        self.progress_bar = ctk.CTkProgressBar(
            self,
            mode="indeterminate",
            height=8,
            corner_radius=4,
            progress_color=("#4CAF50", "#4CAF50"),
            fg_color=("gray85", "gray25")
        )
        self.progress_bar.set(0) # Initialiser √† 0
        
        # Zone d'input
        self.input_frame = ctk.CTkFrame(self, fg_color="transparent")
        self.input_frame.pack(fill="x", padx=20, pady=(0, 20))
        self.input_frame.grid_columnconfigure(0, weight=1)
        
        self.entry = ctk.CTkEntry(
            self.input_frame,
            placeholder_text="Tapez votre message...",
            height=50,
            font=("Arial", 13)
        )
        self.entry.grid(row=0, column=0, sticky="ew", padx=(0, 10))
        self.entry.bind("<Return>", lambda e: self.send_message())
        
        self.btn_send = ctk.CTkButton(
            self.input_frame,
            text="Envoyer",
            width=100,
            height=50,
            corner_radius=25,
            fg_color=("#4CAF50", "#388E3C"),
            hover_color=("#45A049", "#2E7D32"),
            font=("Arial", 13, "bold"),
            command=self.send_message
        )
        self.btn_send.grid(row=0, column=1)
        
        # Message de bienvenue et tests de connexion
        self.add_system_message(f"Connexion √† {self.assistant.get('name')}...")
        if self.assistant.get('description'):
            self.add_system_message(f"Description : {self.assistant.get('description')}")
        
        # Tester les connexions LLM
        self.after(100, self.test_llm_connections)
        
        # Envoyer automatiquement un message de bienvenue au LLM apr√®s les tests
        self.after(2000, self.send_welcome_message)
    
    def test_llm_connections(self):
        """Teste les connexions aux LLM providers (chat et scraping) au lancement."""
        try:
            settings = self.app.data_manager.get_settings()
            
            # Test 1: Provider Chat
            chat_provider = self.assistant.get('provider', 'OpenAI GPT-4o mini')
            chat_api_key = settings.get('api_keys', {}).get(chat_provider)
            
            self.add_system_message(f"üîç Test de connexion au LLM Chat ({chat_provider})...")
            
            if not chat_api_key:
                self.add_system_message(f"‚ö†Ô∏è Aucune cl√© API configur√©e pour {chat_provider}")
            else:
                # Test de connexion
                endpoint = None
                if "IAKA" in chat_provider:
                    endpoint = settings.get('endpoints', {}).get(chat_provider)
                
                success, message = LLMConnectionTester.test_provider(chat_provider, chat_api_key, base_url=endpoint)
                
                if success:
                    self.add_system_message(f"‚úÖ LLM Chat: Connexion r√©ussie √† {chat_provider}")
                else:
                    # Extraire un r√©sum√© de l'erreur
                    if "quota" in message.lower() or "429" in message:
                        error_summary = "Quota d√©pass√©"
                    elif "401" in message or "403" in message or "invalid" in message.lower():
                        error_summary = "Cl√© API invalide"
                    elif "404" in message or "not found" in message.lower():
                        error_summary = "Mod√®le non disponible"
                    else:
                        error_summary = "Erreur de connexion"
                    
                    self.add_system_message(f"‚ùå LLM Chat: {error_summary} ({chat_provider})")
            
            # Test 2: Provider ScrapeGraph (si URL cible configur√©e)
            if self.assistant.get('target_url'):
                sg_provider = settings.get("scrapegraph_provider", "OpenAI GPT-4o mini")
                sg_api_key = settings.get("api_keys", {}).get(sg_provider)
                
                self.add_system_message(f"üîç Test de connexion au LLM Scraping ({sg_provider})...")
                
                if not sg_api_key:
                    self.add_system_message(f"‚ö†Ô∏è Aucune cl√© API configur√©e pour {sg_provider}")
                else:
                    # Test de connexion
                    endpoint = None
                    if "IAKA" in sg_provider:
                        endpoint = settings.get('endpoints', {}).get(sg_provider)
                    
                    success, message = LLMConnectionTester.test_provider(sg_provider, sg_api_key, base_url=endpoint)
                    
                    if success:
                        self.add_system_message(f"‚úÖ LLM Scraping: Connexion r√©ussie √† {sg_provider}")
                    else:
                        # Extraire un r√©sum√© de l'erreur
                        if "quota" in message.lower() or "429" in message:
                            error_summary = "Quota d√©pass√©"
                        elif "401" in message or "403" in message or "invalid" in message.lower():
                            error_summary = "Cl√© API invalide"
                        elif "404" in message or "not found" in message.lower():
                            error_summary = "Mod√®le non disponible"
                        else:
                            error_summary = "Erreur de connexion"
                        
                        self.add_system_message(f"‚ùå LLM Scraping: {error_summary} ({sg_provider})")
            
            self.add_system_message("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            
        except Exception as e:
            self.add_system_message(f"‚ö†Ô∏è Erreur lors des tests de connexion: {str(e)}")
    
    
    def send_welcome_message(self):
        """Envoie automatiquement un message de bienvenue au LLM."""
        if self.assistant.get('target_url'):
            welcome_msg = "Bonjour ! Pr√©sente-toi bri√®vement et lance imm√©diatement la recherche sur le site cible en fonction de ton objectif. IMPORTANT : Respecte scrupuleusement les consignes d√©finies dans tes instructions (Contexte, Objectif, Limites)."
        else:
            welcome_msg = "Bonjour ! Peux-tu te pr√©senter bri√®vement ?"
            
        self.add_user_message(welcome_msg)
        
        # Afficher l'indicateur de chargement
        self.show_loading()
        
        # D√©sactiver le bouton d'envoi
        self.btn_send.configure(state="disabled", text="Envoi...")
        
        # Envoyer la requ√™te au LLM dans un thread s√©par√©
        thread = threading.Thread(target=self._send_to_llm, args=(welcome_msg,))
        thread.daemon = True
        thread.start()
    
    def show_loading(self):
        """Affiche l'indicateur de chargement."""
        self.progress_bar.pack(fill="x", padx=20, pady=(0, 10), before=self.input_frame)
        self.progress_bar.start()
    
    def hide_loading(self):
        """Cache l'indicateur de chargement."""
        self.progress_bar.stop()
        self.progress_bar.pack_forget()
    
    def add_system_message(self, text):
        """Ajoute un message syst√®me."""
        self.history.append({"role": "Syst√®me", "content": text, "timestamp": datetime.datetime.now()})
        self.chat_area.configure(state="normal")
        self.chat_area.insert("end", f"‚ÑπÔ∏è {text}\n\n", "system")
        self.chat_area.tag_config("system", foreground="gray")
        self.chat_area.configure(state="disabled")
        self.chat_area.see("end")
    
    def add_user_message(self, text):
        """Ajoute un message de l'utilisateur."""
        self.history.append({"role": "Utilisateur", "content": text, "timestamp": datetime.datetime.now()})
        self.chat_area.configure(state="normal")
        self.chat_area.insert("end", f"Vous : {text}\n\n", "user")
        self.chat_area.tag_config("user", foreground="#2196F3")
        self.chat_area.configure(state="disabled")
        self.chat_area.see("end")
    
    def add_assistant_message(self, text):
        """Ajoute un message de l'assistant."""
        self.history.append({"role": "Assistant", "content": text, "timestamp": datetime.datetime.now()})
        self.chat_area.configure(state="normal")
        self.chat_area.insert("end", f"{self.assistant.get('name')} : {text}\n\n", "assistant")
        self.chat_area.tag_config("assistant", foreground="#4CAF50")
        self.chat_area.configure(state="disabled")
        self.chat_area.see("end")
    
    def add_error_message(self, text):
        """Ajoute un message d'erreur."""
        self.chat_area.configure(state="normal")
        self.chat_area.insert("end", f"‚ùå Erreur : {text}\n\n", "error")
        self.chat_area.tag_config("error", foreground="#F44336")
        self.chat_area.configure(state="disabled")
        self.chat_area.see("end")
    
    def _truncate_results_for_llm(self, results_text, max_chars=5000):
        """
        Tronque les r√©sultats s'ils sont trop longs pour le contexte du LLM.
        
        Args:
            results_text: Le texte complet des r√©sultats
            max_chars: Nombre maximum de caract√®res √† conserver
            
        Returns:
            Texte tronqu√© avec une note si n√©cessaire
        """
        if len(results_text) <= max_chars:
            return results_text
        
        truncated = results_text[:max_chars]
        total_chars = len(results_text)
        return f"{truncated}\n\n[... R√©sultats tronqu√©s - {total_chars} caract√®res au total, montrant les {max_chars} premiers caract√®res ...]"

    def build_system_prompt(self):
        """Construit le prompt syst√®me avec toutes les informations de l'assistant."""
        parts = []
        
        if self.assistant.get('role'):
            parts.append(f"R√¥le : {self.assistant.get('role')}")
        
        if self.assistant.get('context'):
            parts.append(f"Contexte : {self.assistant.get('context')}")
        
        if self.assistant.get('objective'):
            parts.append(f"Objectif : {self.assistant.get('objective')}")
        
        if self.assistant.get('limits'):
            parts.append(f"Limites : {self.assistant.get('limits')}")
        
        if self.assistant.get('response_format'):
            parts.append(f"Format de r√©ponse : {self.assistant.get('response_format')}")
            
        # Instructions pour l'outil de recherche
        target_url = self.assistant.get('target_url')
        if target_url:
            parts.append(f"""
IMPORTANT : Tu as acc√®s √† un outil de recherche sur le site : {target_url}
Pour effectuer une recherche sur ce site, r√©ponds UNIQUEMENT avec la commande suivante :
ACTION: SEARCH <ta requ√™te de recherche>

Exemple :
Utilisateur : "Cherche des chaussures rouges"
Toi : ACTION: SEARCH chaussures rouges

Je t'enverrai ensuite les r√©sultats de la recherche, et tu pourras formuler ta r√©ponse finale.
N'utilise cette commande que si c'est pertinent pour r√©pondre √† l'utilisateur.
""")
            
            # Instructions d√©taill√©es pour le site
            url_instructions = self.assistant.get('url_instructions')
            if url_instructions:
                parts.append(f"""
INSTRUCTIONS POUR LE SITE {target_url} :
Le syst√®me utilise l'IA pour extraire automatiquement les donn√©es selon ces instructions :
{url_instructions}

Tu n'as pas besoin de g√©rer l'extraction toi-m√™me - l'IA s'en charge.
Concentre-toi sur la formulation de requ√™tes de recherche pertinentes et l'analyse des r√©sultats retourn√©s.
""")

        
        # Consignes de priorit√©
        parts.append("""
IMPORTANT :
1. Tu dois analyser et comprendre le fonctionnement du site internet cible pour naviguer et extraire les informations pertinentes.
2. MAIS SURTOUT : Ta PRIORIT√â ABSOLUE est de respecter scrupuleusement les consignes d√©finies ci-dessus (R√¥le, Contexte, Objectif, Limites).
3. En cas de conflit entre une information du site et tes instructions, tes instructions (Limites notamment) pr√©valent toujours.
""")

        return "\n\n".join(parts) if parts else "Tu es un assistant utile et serviable."
    
    def send_message(self):
        """Envoie un message au LLM."""
        user_message = self.entry.get().strip()
        
        if not user_message:
            return
        
        # Afficher le message de l'utilisateur
        self.add_user_message(user_message)
        self.entry.delete(0, "end")
        
        # Afficher l'indicateur de chargement
        self.show_loading()
        
        # D√©sactiver le bouton d'envoi
        self.btn_send.configure(state="disabled", text="Envoi...")
        
        # Envoyer la requ√™te au LLM dans un thread s√©par√©
        thread = threading.Thread(target=self._send_to_llm, args=(user_message,))
        thread.daemon = True
        thread.start()
    
    def _send_to_llm(self, user_message):
        """Envoie la requ√™te au LLM (dans un thread s√©par√©)."""
        try:
            # R√©cup√©rer la cl√© API
            settings = self.app.data_manager.get_settings()
            provider = self.assistant.get('provider', 'OpenAI GPT-4o mini')
            api_key = settings.get('api_keys', {}).get(provider)
            
            if not api_key:
                self.add_error_message(f"Aucune cl√© API configur√©e pour {provider}. Veuillez configurer votre cl√© dans la page Administration.")
                self.btn_send.configure(state="normal", text="Envoyer")
                return
            
            # Construire le prompt syst√®me
            system_prompt = self.build_system_prompt()
            
            # Appeler le LLM selon le provider
            # Appeler le LLM selon le provider
            if "OpenAI" in provider:
                response_text = self._call_openai(api_key, system_prompt, user_message)
            elif "Gemini" in provider:
                response_text = self._call_gemini(api_key, system_prompt, user_message)
            elif "Claude" in provider:
                response_text = self._call_claude(api_key, system_prompt, user_message)
            elif "Llama" in provider or "Groq" in provider:
                response_text = self._call_groq(api_key, system_prompt, user_message)
            elif "Mistral" in provider:
                response_text = self._call_mistral(api_key, system_prompt, user_message)
            elif "DeepSeek" in provider:
                 # DeepSeek utilise l'API OpenAI avec une base_url sp√©cifique
                 response_text = self._call_openai_compatible(api_key, "https://api.deepseek.com", system_prompt, user_message)
            elif "IAKA" in provider:
                endpoint = settings.get('endpoints', {}).get(provider)
                if not endpoint:
                    raise Exception(f"Endpoint URL non configur√© pour {provider}.")
                response_text = self._call_openai_compatible(api_key, endpoint, system_prompt, user_message)
            else:
                response_text = f"Provider {provider} non support√© pour le moment."
            
            # Traiter la r√©ponse (v√©rifier si action requise)
            self._process_llm_response(response_text, api_key, system_prompt, user_message)
            
        except Exception as e:
            error_msg = str(e)
            
            # D√©tection des erreurs courantes pour un message plus clair
            if "429" in error_msg or "quota" in error_msg.lower():
                friendly_msg = (
                    "‚ö†Ô∏è **Quota API d√©pass√©**\n"
                    "La cl√© API utilis√©e a atteint sa limite.\n"
                    "Solution : Changez de mod√®le (ex: Groq, Gemini) dans la modification de l'assistant."
                )
                self.add_error_message(friendly_msg)
            elif "401" in error_msg or "invalid" in error_msg.lower():
                friendly_msg = (
                    "‚ö†Ô∏è **Cl√© API invalide**\n"
                    "La cl√© API est incorrecte ou a expir√©.\n"
                    "Solution : V√©rifiez la cl√© dans la page Administration."
                )
                self.add_error_message(friendly_msg)
            else:
                self.add_error_message(f"Erreur technique : {error_msg}")
        
        finally:
            # Cacher l'indicateur de chargement
            self.hide_loading()
            
            # R√©activer le bouton d'envoi
            self.btn_send.configure(state="normal", text="Envoyer")

    def _process_llm_response(self, response_text, api_key, system_prompt, original_user_message):
        """Traite la r√©ponse du LLM et g√®re les actions (outils)."""
        
        # V√©rifier si le LLM demande une action de recherche
        if "ACTION: SEARCH" in response_text:
            # S√©parer le message de la commande
            parts = response_text.split("ACTION: SEARCH")
            intro_text = parts[0].strip()
            query = parts[1].strip()
            
            # Afficher le message d'intro s'il y en a un
            if intro_text:
                self.add_assistant_message(intro_text)
            
            self.add_system_message(f"üîé Recherche en cours sur {self.assistant.get('target_url')} : '{query}'...")
            
            # Utiliser l'AI Scraper (simple et intelligent)
            url_instructions = self.assistant.get('url_instructions', '')
            
            if not url_instructions:
                self.add_system_message("‚ö†Ô∏è Aucune instruction d'extraction configur√©e. Veuillez configurer le champ 'Donn√©es √† extraire' dans les param√®tres de l'assistant.")
                return
            
            # Afficher les instructions qui seront utilis√©es
            self.add_system_message(f"üìù Instructions d'extraction:\n{url_instructions}")
            
            # Importer et utiliser l'AI Scraper
            from utils.ai_scraper import AIScraper
            
            try:
                # R√©cup√©rer la configuration ScrapeGraphAI
                settings = self.app.data_manager.get_settings()
                sg_provider = settings.get("scrapegraph_provider", "OpenAI GPT-4o mini")
                sg_api_key = settings.get("api_keys", {}).get(sg_provider)
                
                if not sg_api_key:
                    self.add_system_message(f"‚ö†Ô∏è Aucune cl√© API configur√©e pour le scraping ({sg_provider}). Veuillez v√©rifier la configuration dans Administration.")
                    return

                # Mapper le nom du provider pour AIScraper
                provider_code = "openai"
                if "Gemini" in sg_provider:
                    provider_code = "google"
                elif "Groq" in sg_provider:
                    provider_code = "groq"
                
                # Mapper le mod√®le (simplifi√©)
                model_code = "gpt-4o-mini"
                if "Gemini" in sg_provider:
                    model_code = "gemini-1.5-flash"
                elif "Llama" in sg_provider:
                    model_code = "llama-3.1-8b-instant"
                
                self.add_system_message(f"ü§ñ Scraping avec {sg_provider}...")

                # Cr√©er le scraper IA avec les infos de l'assistant
                ai_scraper = AIScraper(
                    api_key=sg_api_key,
                    model=model_code,
                    provider=provider_code,
                    assistant_id=str(self.assistant.get('id', 'unknown')),
                    assistant_name=self.assistant.get('name', 'Unknown')
                )
                
                # Ex√©cuter la recherche avec l'IA (retourne tuple: results, filepath)
                search_results, results_filepath = ai_scraper.search(
                    url=self.assistant.get('target_url'),
                    query=query,
                    extraction_prompt=url_instructions
                )
                
                # Afficher le chemin du fichier de r√©sultats
                if results_filepath:
                    filename = os.path.basename(results_filepath)
                    self.add_system_message(f"‚úÖ R√©sultats sauvegard√©s: {filename}")
                    
                    # Charger les r√©sultats depuis le fichier pour analyse par le LLM
                    rm = ResultsManager()
                    loaded_results = rm.load_result(results_filepath)
                    
                    if loaded_results:
                        # Extraire les donn√©es importantes
                        results_text = loaded_results.get('results', 'Aucun r√©sultat')
                        
                        # Tronquer si trop long
                        results_text = self._truncate_results_for_llm(results_text)
                        
                        # Message syst√®me pour informer l'utilisateur
                        self.add_system_message("ü§ñ Analyse des r√©sultats en cours...")
                        
                        # Cr√©er un prompt d'analyse structur√© pour le LLM
                        analysis_prompt = f"""Les r√©sultats du scraping ont √©t√© r√©cup√©r√©s avec succ√®s.

REQU√äTE DE RECHERCHE : {query}
URL CIBLE : {self.assistant.get('target_url')}

R√âSULTATS TROUV√âS :
{results_text}

INSTRUCTIONS :
Analyse ces r√©sultats et pr√©sente-les de mani√®re claire, structur√©e et utile pour l'utilisateur.
- Si ce sont des annonces/produits, r√©sume les points principaux de chaque √©l√©ment
- Si ce sont des donn√©es structur√©es, organise-les en liste ou tableau
- Mets en √©vidence les informations les plus pertinentes
- Si aucun r√©sultat n'a √©t√© trouv√©, explique-le clairement et sugg√®re des alternatives
"""
                        
                        # Utiliser le prompt d'analyse au lieu des r√©sultats bruts
                        search_results = results_text
                        new_user_message = analysis_prompt
                    else:
                        # Fallback si le chargement √©choue
                        if len(search_results) > 4000:
                            search_results = search_results[:4000] + "... (tronqu√©)"
                        new_user_message = f"{original_user_message}\n\n[R√âSULTATS DE LA RECHERCHE pour '{query}']:\n{search_results}\n\nUtilise ces informations pour r√©pondre √† la demande initiale."
                else:
                    # Pas de fichier de r√©sultats - utiliser les r√©sultats bruts
                    if len(search_results) > 4000:
                        search_results = search_results[:4000] + "... (tronqu√©)"
                    new_user_message = f"{original_user_message}\n\n[R√âSULTATS DE LA RECHERCHE pour '{query}']:\n{search_results}\n\nUtilise ces informations pour r√©pondre √† la demande initiale."
                
            except Exception as e:
                self.add_system_message(f"‚ùå Erreur lors du scraping IA: {str(e)}")
                logging.error(f"Erreur AI Scraper: {e}")
                return
            
            # Relancer le LLM avec les r√©sultats pour analyse
            
            # Appel r√©cursif (attention √† la boucle infinie, on pourrait ajouter un compteur)
            # Pour simplifier ici, on refait juste un appel standard
            if "OpenAI" in self.assistant.get('provider', ''):
                final_response = self._call_openai(api_key, system_prompt, new_user_message)
            elif "Gemini" in self.assistant.get('provider', ''):
                final_response = self._call_gemini(api_key, system_prompt, new_user_message)
            elif "Claude" in self.assistant.get('provider', ''):
                final_response = self._call_claude(api_key, system_prompt, new_user_message)
            elif "Llama" in self.assistant.get('provider', '') or "Groq" in self.assistant.get('provider', ''):
                final_response = self._call_groq(api_key, system_prompt, new_user_message)
            elif "Mistral" in self.assistant.get('provider', ''):
                final_response = self._call_mistral(api_key, system_prompt, new_user_message)
            elif "DeepSeek" in self.assistant.get('provider', ''):
                 final_response = self._call_openai_compatible(api_key, "https://api.deepseek.com", system_prompt, new_user_message)
            elif "IAKA" in self.assistant.get('provider', ''):
                settings = self.app.data_manager.get_settings()
                endpoint = settings.get('endpoints', {}).get(self.assistant.get('provider', ''))
                final_response = self._call_openai_compatible(api_key, endpoint, system_prompt, new_user_message)
            else:
                final_response = "Erreur: Provider non support√© pour la suite de l'action."
                
            self.add_assistant_message(final_response)
        else:
            # R√©ponse normale
            self.add_assistant_message(response_text)
    
    def _call_openai(self, api_key, system_prompt, user_message):
        """Appelle l'API OpenAI."""
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    def _call_openai_compatible(self, api_key, base_url, system_prompt, user_message):
        """Appelle une API compatible OpenAI (ex: IAKA)."""
        from openai import OpenAI
        client = OpenAI(api_key=api_key, base_url=base_url)
        
        # Essayer de d√©terminer le mod√®le √† utiliser
        # Pour IAKA, on peut essayer un mod√®le par d√©faut ou lister
        try:
            # On tente d'abord avec un nom g√©n√©rique
            model_to_use = "gpt-3.5-turbo"
            
            # Si on peut lister les mod√®les, on prend le premier
            try:
                models = client.models.list()
                if models.data:
                    model_to_use = models.data[0].id
            except:
                pass
                
            response = client.chat.completions.create(
                model=model_to_use,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=500,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Erreur lors de l'appel √† l'API compatible : {str(e)}"

    def _call_gemini(self, api_key, system_prompt, user_message):
        """Appelle l'API Google Gemini."""
        import google.generativeai as genai
        
        genai.configure(api_key=api_key)
        
        # Trouver un mod√®le disponible
        available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                model_name = m.name.lower()
                if 'preview' not in model_name and 'exp' not in model_name:
                    available_models.append(m.name)
        
        if not available_models:
            raise Exception("Aucun mod√®le Gemini disponible")
        
        # Prioriser flash
        flash_models = [m for m in available_models if 'flash' in m.lower()]
        model_name = flash_models[0] if flash_models else available_models[0]
        
        model = genai.GenerativeModel(model_name)
        
        # Combiner system prompt et user message
        full_prompt = f"{system_prompt}\n\nUtilisateur : {user_message}"
        
        response = model.generate_content(full_prompt)
        return response.text
    
    def _call_claude(self, api_key, system_prompt, user_message):
        """Appelle l'API Anthropic Claude."""
        from anthropic import Anthropic
        
        client = Anthropic(api_key=api_key)
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=500,
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_message}
            ]
        )
        
        return response.content[0].text
    
    def _call_groq(self, api_key, system_prompt, user_message):
        """Appelle l'API Groq (Llama)."""
        from groq import Groq
        
        client = Groq(api_key=api_key)
        
        response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    def _call_mistral(self, api_key, system_prompt, user_message):
        """Appelle l'API Mistral."""
        from mistralai.client import MistralClient
        
        client = MistralClient(api_key=api_key)
        
        # Combiner system prompt et user message
        full_message = f"{system_prompt}\n\nUtilisateur : {user_message}"
        
        response = client.chat(
            model="mistral-small-latest",
            messages=[
                {"role": "user", "content": full_message}
            ],
            max_tokens=500
        )
        
        return response.choices[0].message.content

    def export_to_excel(self):
        """Exporte le tableau de la 'Partie 2 : Synth√®se √† exporter' vers Excel."""
        if not self.history:
            messagebox.showinfo("Info", "Aucun message √† exporter.")
            return
            
        # Rechercher la "Partie 2" dans les messages de l'assistant
        target_section = "Partie 2 : Synth√®se √† exporter"
        table_data = None
        
        # Parcourir l'historique √† l'envers pour trouver le dernier message pertinent
        for msg in reversed(self.history):
            if msg["role"] == "Assistant" and target_section in msg["content"]:
                # Extraire le contenu apr√®s le titre de la section
                content = msg["content"]
                start_index = content.find(target_section) + len(target_section)
                section_content = content[start_index:]
                
                # Chercher un tableau Markdown
                table_data = self._parse_markdown_table(section_content)
                if table_data:
                    break
        
        if not table_data:
            messagebox.showwarning("Attention", f"Aucune table trouv√©e dans la section '{target_section}'.\nAssurez-vous que l'assistant a g√©n√©r√© cette section avec un tableau.")
            return

        try:
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
            
            # Demander l'emplacement de sauvegarde
            filename = filedialog.asksaveasfilename(
                defaultextension=".xlsx",
                filetypes=[("Excel files", "*.xlsx"), ("All files", "*.*")],
                initialfile=f"synthese_export_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                title="Exporter la synth√®se"
            )
            
            if not filename:
                return
            
            # Cr√©er le classeur Excel
            wb = Workbook()
            ws = wb.active
            ws.title = "Synth√®se"
            
            # Styles
            header_fill = PatternFill(start_color="4CAF50", end_color="4CAF50", fill_type="solid")
            header_font = Font(bold=True, color="FFFFFF")
            center_align = Alignment(horizontal="center", vertical="center", wrap_text=True)
            thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
            
            # √âcrire les en-t√™tes
            if table_data["headers"]:
                ws.append(table_data["headers"])
                for col_idx, cell in enumerate(ws[1], 1):
                    cell.fill = header_fill
                    cell.font = header_font
                    cell.alignment = center_align
                    cell.border = thin_border
            
            # √âcrire les donn√©es
            for row in table_data["rows"]:
                ws.append(row)
                # Appliquer les bordures et l'alignement √† la derni√®re ligne ajout√©e
                for cell in ws[ws.max_row]:
                    cell.border = thin_border
                    cell.alignment = Alignment(vertical="center", wrap_text=True)
            
            # Ajuster la largeur des colonnes
            for col in ws.columns:
                max_length = 0
                column = col[0].column_letter # Get the column name
                for cell in col:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = (max_length + 2)
                # Limiter la largeur max pour √©viter des colonnes g√©antes
                ws.column_dimensions[column].width = min(adjusted_width, 50)
            
            # Sauvegarder
            wb.save(filename)
            messagebox.showinfo("Succ√®s", f"Synth√®se export√©e avec succ√®s vers :\n{filename}")
            
        except ImportError:
            messagebox.showerror("Erreur", "Le module 'openpyxl' est manquant. Veuillez l'installer.")
        except Exception as e:
            messagebox.showerror("Erreur", f"Une erreur est survenue lors de l'export :\n{str(e)}")

    def _parse_markdown_table(self, text):
        """Parse un tableau Markdown dans le texte donn√©."""
        lines = text.strip().split('\n')
        headers = []
        rows = []
        in_table = False
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # D√©tection du d√©but de tableau (ligne avec des |)
            if "|" in line:
                # Nettoyer la ligne (enlever les | de d√©but et fin si pr√©sents)
                parts = [p.strip() for p in line.split('|') if p.strip()]
                
                if not parts:
                    continue
                    
                if not in_table:
                    # Potentiellement les en-t√™tes
                    # V√©rifier si la ligne suivante est une ligne de s√©paration (---)
                    if i + 1 < len(lines) and "---" in lines[i+1]:
                        headers = parts
                        in_table = True
                        # Sauter la ligne de s√©paration
                        continue
                elif "---" in line:
                    # Ligne de s√©paration, on ignore
                    continue
                else:
                    # Ligne de donn√©es
                    rows.append(parts)
            elif in_table and not line:
                # Fin du tableau si ligne vide
                break
        
        if headers or rows:
            return {"headers": headers, "rows": rows}
        return None
