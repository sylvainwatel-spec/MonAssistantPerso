"""
AI-powered web scraper using ScrapeGraphAI.
Simplifie le scraping en utilisant des prompts en langage naturel au lieu de s√©lecteurs CSS.
"""

import logging
import traceback
from typing import Any, Dict, Optional, Union

# Patch for ScrapeGraphAI compatibility
try:
    import utils.patch_langchain
except ImportError:
    pass

from scrapegraphai.graphs import SmartScraperGraph


class AIScraper:
    """
    Scraper intelligent utilisant l'IA pour extraire des donn√©es de sites web.
    Pas besoin de s√©lecteurs CSS - d√©crivez simplement ce que vous voulez en fran√ßais.
    """
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", provider: str = "openai") -> None:
        """
        Initialise le scraper IA.
        
        Args:
            api_key: Cl√© API pour le LLM
            model: Mod√®le √† utiliser (gpt-4o-mini, gemini-pro, etc.)
            provider: Fournisseur LLM (openai, google, groq, etc.)
        """
        self.api_key = api_key
        self.model = model
        self.provider = provider
        self.logger = logging.getLogger(__name__)
    
    def search(self, url: str, query: str, extraction_prompt: str) -> Union[str, Dict[str, Any]]:
        """
        Effectue une recherche intelligente sur un site web.
        
        Args:
            url: URL du site (ex: https://www.leboncoin.fr)
            query: Requ√™te de recherche (ex: "v√©los √©lectriques Paris")
            extraction_prompt: Description de ce qu'on veut extraire
        
        Returns:
            Donn√©es extraites en format structur√© ou message d'erreur.
        """
        try:
            # Construire l'URL de recherche
            if "?" in url:
                search_url = f"{url}&q={query}"
            else:
                search_url = f"{url}/recherche?text={query}"
            
            self.logger.info(f"Scraping URL: {search_url}")
            self.logger.info(f"Extraction prompt: {extraction_prompt}")
            
            # Configuration du scraper
            graph_config = {
                "llm": {
                    "api_key": self.api_key,
                    "model": self.model,
                },
                "verbose": True,
                "headless": False,  # Navigateur visible pour debugging
            }
            
            # Adaptation pour les providers sp√©cifiques
            if "google" in self.provider.lower() or "gemini" in self.provider.lower():
                graph_config["llm"]["model"] = f"gemini/{self.model}"
            elif "groq" in self.provider.lower():
                graph_config["llm"]["model"] = f"groq/{self.model}"
            elif "openai" in self.provider.lower():
                graph_config["llm"]["model"] = f"openai/{self.model}"
            # Par d√©faut, on laisse tel quel (souvent interpr√©t√© comme OpenAI)
            
            # Cr√©er le scraper intelligent
            scraper = SmartScraperGraph(
                prompt=extraction_prompt,
                source=search_url,
                config=graph_config
            )
            
            # Ex√©cuter le scraping
            self.logger.info("D√©marrage du scraping avec IA...")
            result = scraper.run()
            
            self.logger.info(f"Scraping termin√©. R√©sultat: {result}")
            
            # Formater le r√©sultat pour l'affichage
            if isinstance(result, dict):
                return self._format_result(result)
            else:
                return str(result)
                
        except Exception as e:
            error_str = str(e)
            self.logger.error(f"Erreur lors du scraping IA: {e}")
            traceback.print_exc()
            
            # D√©tection sp√©cifique des erreurs de quota
            if "quota" in error_str.lower() or "429" in error_str:
                return (
                    "‚ùå **Quota API d√©pass√©**\n\n"
                    "Votre cl√© OpenAI a atteint sa limite de quota.\n\n"
                    "**Solutions** :\n"
                    "1. Ajoutez des cr√©dits sur votre compte OpenAI : https://platform.openai.com/account/billing\n"
                    "2. Ou changez de provider LLM (Gemini, Groq, etc.) dans la page Administration\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # D√©tection des erreurs d'authentification
            elif "401" in error_str or "invalid" in error_str.lower() and "key" in error_str.lower():
                return (
                    "‚ùå **Cl√© API invalide**\n\n"
                    "Votre cl√© API n'est pas reconnue ou a expir√©.\n\n"
                    "Veuillez v√©rifier votre cl√© dans la page Administration.\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # Erreur g√©n√©rique
            return (
                f"‚ùå **Erreur lors du scraping** : {error_str}\n\n"
                "**V√©rifiez que** :\n"
                "- L'URL est accessible et correcte\n"
                "- Le prompt d'extraction est clair et pr√©cis\n"
                "- Votre cl√© API est valide et a du cr√©dit disponible"
            )
    
    def _format_result(self, result: Union[Dict[str, Any], str]) -> str:
        """
        Formate le r√©sultat JSON en texte lisible.
        
        Args:
            result: R√©sultat du scraping
            
        Returns:
            R√©sultat format√©
        """
        if not result:
            return "Aucun r√©sultat trouv√©."
        
        # Si le r√©sultat contient une liste d'items
        if isinstance(result, dict) and any(isinstance(v, list) for v in result.values()):
            formatted = "üìä R√©sultats extraits:\n\n"
            
            for key, value in result.items():
                if isinstance(value, list):
                    formatted += f"**{key.upper()}**:\n"
                    for i, item in enumerate(value, 1):
                        if isinstance(item, dict):
                            formatted += f"\n  {i}. "
                            for field, field_value in item.items():
                                formatted += f"{field}: {field_value} | "
                            formatted = formatted.rstrip(" | ") + "\n"
                        else:
                            formatted += f"  {i}. {item}\n"
                    formatted += "\n"
                else:
                    formatted += f"**{key}**: {value}\n"
            
            return formatted.strip()
        
        # Si le r√©sultat est un simple dictionnaire
        elif isinstance(result, dict):
            formatted = "üìä R√©sultat extrait:\n\n"
            for key, value in result.items():
                formatted += f"**{key}**: {value}\n"
            return formatted.strip()
        
        # Sinon, retourner tel quel
        return str(result)
    
    def simple_scrape(self, url: str, extraction_prompt: str) -> Union[str, Dict[str, Any]]:
        """
        Scrape simple d'une page sans recherche.
        
        Args:
            url: URL compl√®te de la page
            extraction_prompt: Description de ce qu'on veut extraire
            
        Returns:
            Donn√©es extraites
        """
        try:
            self.logger.info(f"Scraping simple de: {url}")
            self.logger.info(f"Extraction prompt: {extraction_prompt}")
            
            # Configuration du scraper
            graph_config = {
                "llm": {
                    "api_key": self.api_key,
                    "model": self.model,
                },
                "verbose": True,
                "headless": False,
            }
            
            # Adaptation pour les providers sp√©cifiques
            if "google" in self.provider.lower() or "gemini" in self.provider.lower():
                graph_config["llm"]["model"] = f"gemini/{self.model}"
            elif "groq" in self.provider.lower():
                graph_config["llm"]["model"] = f"groq/{self.model}"
            elif "openai" in self.provider.lower():
                graph_config["llm"]["model"] = f"openai/{self.model}"
            # Par d√©faut, on laisse tel quel
            
            scraper = SmartScraperGraph(
                prompt=extraction_prompt,
                source=url,
                config=graph_config
            )
            
            result = scraper.run()
            
            if isinstance(result, dict):
                return self._format_result(result)
            else:
                return str(result)
                
        except Exception as e:
            error_str = str(e)
            self.logger.error(f"Erreur lors du scraping simple: {e}")
            traceback.print_exc()
            
            # D√©tection sp√©cifique des erreurs de quota
            if "quota" in error_str.lower() or "429" in error_str:
                return (
                    "‚ùå **Quota API d√©pass√©**\n\n"
                    "Votre cl√© OpenAI a atteint sa limite de quota.\n\n"
                    "**Solutions** :\n"
                    "1. Ajoutez des cr√©dits sur votre compte OpenAI : https://platform.openai.com/account/billing\n"
                    "2. Ou changez de provider LLM (Gemini, Groq, etc.) dans la page Administration\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # D√©tection des erreurs d'authentification
            elif "401" in error_str or "invalid" in error_str.lower() and "key" in error_str.lower():
                return (
                    "‚ùå **Cl√© API invalide**\n\n"
                    "Votre cl√© API n'est pas reconnue ou a expir√©.\n\n"
                    "Veuillez v√©rifier votre cl√© dans la page Administration.\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # Erreur g√©n√©rique
            return (
                f"‚ùå **Erreur lors du scraping** : {error_str}\n\n"
                "**V√©rifiez que** :\n"
                "- L'URL est accessible et correcte\n"
                "- Le prompt d'extraction est clair et pr√©cis\n"
                "- Votre cl√© API est valide et a du cr√©dit disponible"
            )
