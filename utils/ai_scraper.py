"""
AI-powered web scraper using ScrapeGraphAI.
Simplifie le scraping en utilisant des prompts en langage naturel au lieu de s√©lecteurs CSS.
"""

import logging
import traceback
from typing import Any, Dict, Optional, Union, Tuple
from utils.results_manager import ResultsManager

# Patch for ScrapeGraphAI compatibility
try:
    import utils.patch_langchain
except ImportError:
    pass

from scrapegraphai.graphs import SmartScraperGraph


class AIScraper:
    """
    Scraper intelligent utilisant l'IA pour extraire des donn√©es de sites web.
    Pas besoin de s√©lecteurs CSS - d√©crivez simplement ce que vous voulez en fran√ßais.
    """
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", provider: str = "openai", assistant_id: str = None, assistant_name: str = None) -> None:
        """
        Initialise le scraper IA.
        
        Args:
            api_key: Cl√© API pour le LLM
            model: Mod√®le √† utiliser (gpt-4o-mini, gemini-pro, etc.)
            provider: Fournisseur LLM (openai, google, groq, etc.)
            assistant_id: ID de l'assistant (pour sauvegarder les r√©sultats)
            assistant_name: Nom de l'assistant (pour sauvegarder les r√©sultats)
        """
        self.api_key = api_key
        self.model = model
        self.provider = provider
        self.assistant_id = assistant_id
        self.assistant_name = assistant_name
        self.logger = logging.getLogger(__name__)
        self.results_manager = ResultsManager()
    
    def search(self, url: str, query: str, extraction_prompt: str) -> Tuple[Union[str, Dict[str, Any]], Optional[str]]:
        """
        Effectue une recherche intelligente sur un site web.
        
        Args:
            url: URL du site (ex: https://www.leboncoin.fr)
            query: Requ√™te de recherche (ex: "v√©los √©lectriques Paris")
            extraction_prompt: Description de ce qu'on veut extraire
        
        Returns:
            Tuple (r√©sultats format√©s, chemin du fichier de sauvegarde)
        """
        try:
            # Construire l'URL de recherche
            if "?" in url:
                search_url = f"{url}&q={query}"
            else:
                search_url = f"{url}/recherche?text={query}"
            
            self.logger.info(f"Scraping URL: {search_url}")
            self.logger.info(f"Extraction prompt: {extraction_prompt}")
            
            # Configuration du scraper
            graph_config = {
                "llm": {
                    "api_key": self.api_key,
                    "model": self.model,
                },
                "verbose": True,
                "headless": False,  # Navigateur visible pour debugging
            }
            
            # Adaptation pour les providers sp√©cifiques
            if "google" in self.provider.lower() or "gemini" in self.provider.lower():
                graph_config["llm"]["model"] = f"gemini/{self.model}"
            elif "groq" in self.provider.lower():
                graph_config["llm"]["model"] = f"groq/{self.model}"
            elif "openai" in self.provider.lower():
                graph_config["llm"]["model"] = f"openai/{self.model}"
            # Par d√©faut, on laisse tel quel (souvent interpr√©t√© comme OpenAI)
            
            # Cr√©er le scraper intelligent
            scraper = SmartScraperGraph(
                prompt=extraction_prompt,
                source=search_url,
                config=graph_config
            )
            
            # Ex√©cuter le scraping
            self.logger.info("D√©marrage du scraping avec IA...")
            result = scraper.run()
            
            self.logger.info(f"Scraping termin√©. R√©sultat: {result}")
            
            # Formater le r√©sultat pour l'affichage
            if isinstance(result, dict):
                formatted_result = self._format_result(result)
            else:
                formatted_result = str(result)
            
            # Sauvegarder les r√©sultats
            filepath = self._save_scraping_result(
                url=search_url,
                query=query,
                extraction_prompt=extraction_prompt,
                raw_results=result,
                formatted_results=formatted_result
            )
            
            return formatted_result, filepath
                
        except Exception as e:
            error_str = str(e)
            self.logger.error(f"Erreur lors du scraping IA: {e}")
            traceback.print_exc()
            
            # D√©tection sp√©cifique des erreurs de limite de tokens (413)
            if "413" in error_str or "rate_limit_exceeded" in error_str.lower() or "request too large" in error_str.lower():
                return (
                    "‚ùå **Requ√™te trop volumineuse**\n\n"
                    "Le contenu de la page d√©passe la limite de tokens du mod√®le.\n\n"
                    "**Solutions** :\n"
                    "1. Utilisez un mod√®le avec une limite plus √©lev√©e (ex: GPT-4o mini)\n"
                    "2. R√©duisez la taille de votre prompt d'extraction\n"
                    "3. Ciblez une URL plus sp√©cifique avec moins de contenu\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # D√©tection sp√©cifique des erreurs de quota
            elif "quota" in error_str.lower() or "429" in error_str:
                return (
                    "‚ùå **Quota API d√©pass√©**\n\n"
                    "Votre cl√© OpenAI a atteint sa limite de quota.\n\n"
                    "**Solutions** :\n"
                    "1. Ajoutez des cr√©dits sur votre compte OpenAI : https://platform.openai.com/account/billing\n"
                    "2. Ou changez de provider LLM (Gemini, Groq, etc.) dans la page Administration\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # D√©tection des erreurs d'authentification
            elif "401" in error_str or "invalid" in error_str.lower() and "key" in error_str.lower():
                return (
                    "‚ùå **Cl√© API invalide**\n\n"
                    "Votre cl√© API n'est pas reconnue ou a expir√©.\n\n"
                    "Veuillez v√©rifier votre cl√© dans la page Administration.\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # Erreur g√©n√©rique
            error_message = (
                f"‚ùå **Erreur lors du scraping** : {error_str}\n\n"
                "**V√©rifiez que** :\n"
                "- L'URL est accessible et correcte\n"
                "- Le prompt d'extraction est clair et pr√©cis\n"
                "- Votre cl√© API est valide et a du cr√©dit disponible"
            )
            return error_message, None
    
    def _format_result(self, result: Union[Dict[str, Any], str]) -> str:
        """
        Formate le r√©sultat JSON en texte lisible.
        
        Args:
            result: R√©sultat du scraping
            
        Returns:
            R√©sultat format√©
        """
        if not result:
            return "Aucun r√©sultat trouv√©."
        
        # Si le r√©sultat contient une liste d'items
        if isinstance(result, dict) and any(isinstance(v, list) for v in result.values()):
            formatted = "üìä R√©sultats extraits:\n\n"
            
            for key, value in result.items():
                if isinstance(value, list):
                    formatted += f"**{key.upper()}**:\n"
                    for i, item in enumerate(value, 1):
                        if isinstance(item, dict):
                            formatted += f"\n  {i}. "
                            for field, field_value in item.items():
                                formatted += f"{field}: {field_value} | "
                            formatted = formatted.rstrip(" | ") + "\n"
                        else:
                            formatted += f"  {i}. {item}\n"
                    formatted += "\n"
                else:
                    formatted += f"**{key}**: {value}\n"
            
            return formatted.strip()
        
        # Si le r√©sultat est un simple dictionnaire
        elif isinstance(result, dict):
            formatted = "üìä R√©sultat extrait:\n\n"
            for key, value in result.items():
                formatted += f"**{key}**: {value}\n"
            return formatted.strip()
        
        # Sinon, retourner tel quel
        return str(result)
    
    def _save_scraping_result(self, url: str, query: str, extraction_prompt: str, 
                              raw_results: Any, formatted_results: str) -> Optional[str]:
        """
        Sauvegarde les r√©sultats de scraping dans un fichier JSON.
        
        Args:
            url: URL scrap√©e
            query: Requ√™te de recherche
            extraction_prompt: Prompt d'extraction utilis√©
            raw_results: R√©sultats bruts du scraper
            formatted_results: R√©sultats format√©s pour affichage
        
        Returns:
            Chemin du fichier cr√©√©, ou None si erreur
        """
        try:
            data = {
                "assistant_id": self.assistant_id or "unknown",
                "assistant_name": self.assistant_name or "Unknown Assistant",
                "url": url,
                "query": query,
                "extraction_prompt": extraction_prompt,
                "results": formatted_results,
                "raw_results": raw_results,
                "provider": self.provider,
                "model": self.model
            }
            
            filepath = self.results_manager.save_result(data)
            self.logger.info(f"R√©sultats sauvegard√©s dans: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"Erreur lors de la sauvegarde des r√©sultats: {e}")
            return None
    
    def simple_scrape(self, url: str, extraction_prompt: str) -> Tuple[Union[str, Dict[str, Any]], Optional[str]]:
        """
        Scrape simple d'une page sans recherche.
        
        Args:
            url: URL compl√®te de la page
            extraction_prompt: Description de ce qu'on veut extraire
            
        Returns:
            Tuple (r√©sultats format√©s, chemin du fichier de sauvegarde)
        """
        try:
            self.logger.info(f"Scraping simple de: {url}")
            self.logger.info(f"Extraction prompt: {extraction_prompt}")
            
            # Configuration du scraper
            graph_config = {
                "llm": {
                    "api_key": self.api_key,
                    "model": self.model,
                },
                "verbose": True,
                "headless": False,
            }
            
            # Adaptation pour les providers sp√©cifiques
            if "google" in self.provider.lower() or "gemini" in self.provider.lower():
                graph_config["llm"]["model"] = f"gemini/{self.model}"
            elif "groq" in self.provider.lower():
                graph_config["llm"]["model"] = f"groq/{self.model}"
            elif "openai" in self.provider.lower():
                graph_config["llm"]["model"] = f"openai/{self.model}"
            # Par d√©faut, on laisse tel quel
            
            scraper = SmartScraperGraph(
                prompt=extraction_prompt,
                source=url,
                config=graph_config
            )
            
            result = scraper.run()
            
            # Formater le r√©sultat
            if isinstance(result, dict):
                formatted_result = self._format_result(result)
            else:
                formatted_result = str(result)
            
            # Sauvegarder les r√©sultats
            filepath = self._save_scraping_result(
                url=url,
                query="",  # Pas de query pour simple_scrape
                extraction_prompt=extraction_prompt,
                raw_results=result,
                formatted_results=formatted_result
            )
            
            return formatted_result, filepath
                
        except Exception as e:
            error_str = str(e)
            self.logger.error(f"Erreur lors du scraping simple: {e}")
            traceback.print_exc()
            
            # D√©tection sp√©cifique des erreurs de limite de tokens (413)
            if "413" in error_str or "rate_limit_exceeded" in error_str.lower() or "request too large" in error_str.lower():
                return (
                    "‚ùå **Requ√™te trop volumineuse**\n\n"
                    "Le contenu de la page d√©passe la limite de tokens du mod√®le.\n\n"
                    "**Solutions** :\n"
                    "1. Utilisez un mod√®le avec une limite plus √©lev√©e (ex: GPT-4o mini)\n"
                    "2. R√©duisez la taille de votre prompt d'extraction\n"
                    "3. Ciblez une URL plus sp√©cifique avec moins de contenu\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # D√©tection sp√©cifique des erreurs de quota
            elif "quota" in error_str.lower() or "429" in error_str:
                return (
                    "‚ùå **Quota API d√©pass√©**\n\n"
                    "Votre cl√© OpenAI a atteint sa limite de quota.\n\n"
                    "**Solutions** :\n"
                    "1. Ajoutez des cr√©dits sur votre compte OpenAI : https://platform.openai.com/account/billing\n"
                    "2. Ou changez de provider LLM (Gemini, Groq, etc.) dans la page Administration\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # D√©tection des erreurs d'authentification
            elif "401" in error_str or "invalid" in error_str.lower() and "key" in error_str.lower():
                return (
                    "‚ùå **Cl√© API invalide**\n\n"
                    "Votre cl√© API n'est pas reconnue ou a expir√©.\n\n"
                    "Veuillez v√©rifier votre cl√© dans la page Administration.\n\n"
                    f"D√©tails : {error_str}"
                )
            
            # Erreur g√©n√©rique
            error_message = (
                f"‚ùå **Erreur lors du scraping** : {error_str}\n\n"
                "**V√©rifiez que** :\n"
                "- L'URL est accessible et correcte\n"
                "- Le prompt d'extraction est clair et pr√©cis\n"
                "- Votre cl√© API est valide et a du cr√©dit disponible"
            )
            return error_message, None
